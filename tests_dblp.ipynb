{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robin/uu/thesis/convolution-matching/.venvconv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:The OGB package is out of date. Your version is 1.3.4, while the latest version is 1.3.6.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from GraphSummarizers.Coarsener.HeteroCoarsener import HeteroCoarsener\n",
    "from Datasets.NodeClassification.DBLP import DBLP\n",
    "from Datasets.NodeClassification.AIFB import AIFB\n",
    "from Datasets.NodeClassification.TestHetero import TestHeteroSmall, TestHeteroBig\n",
    "import importlib\n",
    "import torch\n",
    "from test_data_converter import dgl_to_pyg_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start create H\n",
      "created H 24.792195320129395\n",
      "stop init costs 16.11068630218506\n",
      "stop intersection 2.196542501449585\n",
      "author authortopaper paper\n",
      "vectorized 4.383763074874878\n",
      "costs 6.541147708892822\n",
      "conference conferencetopaper paper\n",
      "vectorized 0.017961978912353516\n",
      "costs 0.03050994873046875\n",
      "paper papertoauthor author\n",
      "vectorized 3.5703675746917725\n",
      "costs 32.529494524002075\n",
      "paper papertoconference conference\n",
      "vectorized 4.061182975769043\n",
      "costs 34.895891189575195\n",
      "paper papertoterm term\n",
      "vectorized 4.3684327602386475\n",
      "costs 34.54208517074585\n",
      "term termtopaper paper\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = DBLP() \n",
    "original_graph = dataset.load_graph()\n",
    "\n",
    "coarsener = HeteroCoarsener(None,original_graph, 0.5, num_nearest_per_etype=10, num_nearest_neighbors=10,pairs_per_level=10)\n",
    "coarsener.init_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop merging nodes 0.11863064765930176\n",
      "update merge graph 0.12376189231872559\n",
      "stop merging nodes 0.11927294731140137\n",
      "update merge graph 0.044219017028808594\n",
      "stop merging nodes 0.5295741558074951\n",
      "update merge graph 0.7402653694152832\n",
      "stop merging nodes 0.09511899948120117\n",
      "update merge graph 0.23154282569885254\n",
      "stop lowest cost edges 0.0074291229248046875\n",
      "stop merging nodes 0.12328052520751953\n",
      "update merge graph 0.13301777839660645\n",
      "stop merging nodes 0.1815052032470703\n",
      "update merge graph 0.014914512634277344\n",
      "stop merging nodes 0.592731237411499\n",
      "update merge graph 1.0333969593048096\n",
      "stop merging nodes 0.11525106430053711\n",
      "update merge graph 0.16469883918762207\n",
      "stop lowest cost edges 0.007597923278808594\n",
      "stop merging nodes 0.1804368495941162\n",
      "update merge graph 0.1400911808013916\n",
      "stop merging nodes 0.5612847805023193\n",
      "update merge graph 0.8855133056640625\n",
      "stop merging nodes 0.09919476509094238\n",
      "update merge graph 0.17580866813659668\n",
      "stop lowest cost edges 0.006848335266113281\n",
      "stop merging nodes 0.12125062942504883\n",
      "update merge graph 0.1469435691833496\n",
      "stop merging nodes 0.5612735748291016\n",
      "update merge graph 0.7822256088256836\n",
      "stop merging nodes 0.12772607803344727\n",
      "update merge graph 0.1682262420654297\n",
      "stop lowest cost edges 0.005963563919067383\n",
      "stop merging nodes 0.12448906898498535\n",
      "update merge graph 0.1299881935119629\n",
      "stop merging nodes 0.6273040771484375\n",
      "update merge graph 0.813218355178833\n",
      "stop merging nodes 0.09343457221984863\n",
      "update merge graph 0.16670918464660645\n",
      "stop lowest cost edges 0.006418704986572266\n",
      "stop merging nodes 0.11305880546569824\n",
      "update merge graph 0.13205480575561523\n",
      "stop merging nodes 0.5829641819000244\n",
      "update merge graph 1.0204753875732422\n",
      "stop merging nodes 0.12191033363342285\n",
      "update merge graph 0.15958523750305176\n",
      "stop lowest cost edges 0.007108211517333984\n",
      "stop merging nodes 0.11615729331970215\n",
      "update merge graph 0.13654088973999023\n",
      "stop merging nodes 0.5638518333435059\n",
      "update merge graph 0.9263262748718262\n",
      "stop merging nodes 0.09110593795776367\n",
      "update merge graph 0.14244484901428223\n",
      "stop lowest cost edges 0.0068073272705078125\n",
      "stop merging nodes 0.11516523361206055\n",
      "update merge graph 0.12710928916931152\n",
      "stop merging nodes 0.5217177867889404\n",
      "update merge graph 0.9649171829223633\n",
      "stop merging nodes 0.09461688995361328\n",
      "update merge graph 0.14136624336242676\n",
      "stop lowest cost edges 0.007882118225097656\n",
      "stop merging nodes 0.12876105308532715\n",
      "update merge graph 0.13596463203430176\n",
      "stop merging nodes 0.5618672370910645\n",
      "update merge graph 0.8753058910369873\n",
      "stop merging nodes 0.10356569290161133\n",
      "update merge graph 0.15917038917541504\n",
      "stop lowest cost edges 0.006990909576416016\n",
      "stop merging nodes 0.1180267333984375\n",
      "update merge graph 0.13833093643188477\n",
      "stop merging nodes 0.6310567855834961\n",
      "update merge graph 0.8208596706390381\n",
      "stop merging nodes 0.10453224182128906\n",
      "update merge graph 0.14401841163635254\n",
      "stop lowest cost edges 0.006847858428955078\n",
      "stop merging nodes 0.11576962471008301\n",
      "update merge graph 0.11766552925109863\n",
      "stop merging nodes 0.5246677398681641\n",
      "update merge graph 0.7468991279602051\n",
      "stop merging nodes 0.1139075756072998\n",
      "update merge graph 0.14677691459655762\n",
      "stop lowest cost edges 0.006880998611450195\n",
      "stop merging nodes 0.12805986404418945\n",
      "update merge graph 0.16274309158325195\n",
      "stop merging nodes 0.5437047481536865\n",
      "update merge graph 0.9349358081817627\n",
      "stop merging nodes 0.11064434051513672\n",
      "update merge graph 0.1528151035308838\n",
      "stop lowest cost edges 0.0058345794677734375\n",
      "stop merging nodes 0.11689114570617676\n",
      "update merge graph 0.1479487419128418\n",
      "stop merging nodes 0.5265300273895264\n",
      "update merge graph 0.7682459354400635\n",
      "stop merging nodes 0.10113525390625\n",
      "update merge graph 0.13909411430358887\n",
      "stop lowest cost edges 0.007613182067871094\n",
      "stop merging nodes 0.12911319732666016\n",
      "update merge graph 0.16240954399108887\n",
      "stop merging nodes 0.5711719989776611\n",
      "update merge graph 0.8924143314361572\n",
      "stop merging nodes 0.10152578353881836\n",
      "update merge graph 0.14157986640930176\n",
      "stop lowest cost edges 0.0067822933197021484\n",
      "stop merging nodes 0.1308460235595703\n",
      "update merge graph 0.18120098114013672\n",
      "stop merging nodes 0.5407381057739258\n",
      "update merge graph 0.9217793941497803\n",
      "stop merging nodes 0.09901833534240723\n",
      "update merge graph 0.1280076503753662\n",
      "stop lowest cost edges 0.00656580924987793\n",
      "stop merging nodes 0.13138103485107422\n",
      "update merge graph 0.18906950950622559\n",
      "stop merging nodes 0.5320014953613281\n",
      "update merge graph 0.769524335861206\n",
      "stop merging nodes 0.0944054126739502\n",
      "update merge graph 0.14414119720458984\n",
      "stop lowest cost edges 0.0064618587493896484\n",
      "stop merging nodes 0.13755130767822266\n",
      "update merge graph 0.1714785099029541\n",
      "stop merging nodes 0.556166410446167\n",
      "update merge graph 0.8034889698028564\n",
      "stop merging nodes 0.09075117111206055\n",
      "update merge graph 0.10608768463134766\n",
      "stop lowest cost edges 0.007158994674682617\n",
      "stop merging nodes 0.11877131462097168\n",
      "update merge graph 0.181779146194458\n",
      "stop merging nodes 0.5528848171234131\n",
      "update merge graph 0.9372310638427734\n",
      "stop merging nodes 0.08984541893005371\n",
      "update merge graph 0.11542558670043945\n",
      "stop lowest cost edges 0.00682520866394043\n",
      "stop merging nodes 0.12133264541625977\n",
      "update merge graph 0.16808795928955078\n",
      "stop merging nodes 0.5906338691711426\n",
      "update merge graph 1.0179171562194824\n",
      "stop merging nodes 0.09064054489135742\n",
      "update merge graph 0.1202538013458252\n",
      "stop lowest cost edges 0.007317304611206055\n",
      "stop merging nodes 0.11371994018554688\n",
      "update merge graph 0.14183306694030762\n",
      "stop merging nodes 0.5737454891204834\n",
      "update merge graph 1.247128963470459\n",
      "stop merging nodes 0.1006765365600586\n",
      "update merge graph 0.12360525131225586\n",
      "stop lowest cost edges 0.007824897766113281\n",
      "stop merging nodes 0.11989426612854004\n",
      "update merge graph 0.1677408218383789\n",
      "stop merging nodes 0.605743408203125\n",
      "update merge graph 1.5423388481140137\n",
      "stop merging nodes 0.16899824142456055\n",
      "update merge graph 0.12124109268188477\n",
      "stop lowest cost edges 0.007878780364990234\n",
      "stop merging nodes 0.14591360092163086\n",
      "update merge graph 0.19816875457763672\n",
      "stop merging nodes 0.5799846649169922\n",
      "update merge graph 0.8828914165496826\n",
      "stop merging nodes 0.09792971611022949\n",
      "update merge graph 0.13322877883911133\n",
      "stop lowest cost edges 0.006650447845458984\n",
      "stop merging nodes 0.12736082077026367\n",
      "update merge graph 0.16654419898986816\n",
      "stop merging nodes 0.5252320766448975\n",
      "update merge graph 0.9559152126312256\n",
      "stop merging nodes 0.10158443450927734\n",
      "update merge graph 0.11533546447753906\n",
      "stop lowest cost edges 0.00946187973022461\n",
      "stop merging nodes 0.13003826141357422\n",
      "update merge graph 0.17177534103393555\n",
      "stop merging nodes 0.5251941680908203\n",
      "update merge graph 0.9642796516418457\n",
      "stop merging nodes 0.15043234825134277\n",
      "update merge graph 0.12727999687194824\n",
      "stop lowest cost edges 0.006922721862792969\n",
      "stop merging nodes 0.12033534049987793\n",
      "update merge graph 0.16755890846252441\n",
      "stop merging nodes 0.5532300472259521\n",
      "update merge graph 1.122185468673706\n",
      "stop merging nodes 0.11069321632385254\n",
      "update merge graph 0.13443779945373535\n",
      "stop lowest cost edges 0.0077664852142333984\n",
      "stop merging nodes 0.12811994552612305\n",
      "update merge graph 0.15146613121032715\n",
      "stop merging nodes 0.5401878356933594\n",
      "update merge graph 1.057600498199463\n",
      "stop merging nodes 0.09794425964355469\n",
      "update merge graph 0.11233019828796387\n",
      "stop lowest cost edges 0.007872343063354492\n",
      "stop merging nodes 0.11671972274780273\n",
      "update merge graph 0.1881723403930664\n",
      "stop merging nodes 0.5700702667236328\n",
      "update merge graph 1.1670639514923096\n",
      "stop merging nodes 0.10474538803100586\n",
      "update merge graph 0.13092565536499023\n",
      "stop lowest cost edges 0.005952358245849609\n",
      "stop merging nodes 0.11954927444458008\n",
      "update merge graph 0.15411806106567383\n",
      "stop merging nodes 0.6030960083007812\n",
      "update merge graph 1.309880018234253\n",
      "stop merging nodes 0.11142349243164062\n",
      "update merge graph 0.14549040794372559\n",
      "stop lowest cost edges 0.007164716720581055\n",
      "stop merging nodes 0.12552309036254883\n",
      "update merge graph 0.1671757698059082\n",
      "stop merging nodes 0.5420675277709961\n",
      "update merge graph 1.4126691818237305\n",
      "stop merging nodes 0.10275983810424805\n",
      "update merge graph 0.14944148063659668\n",
      "stop lowest cost edges 0.005957603454589844\n",
      "stop merging nodes 0.12100505828857422\n",
      "update merge graph 0.23047614097595215\n",
      "stop merging nodes 0.5713198184967041\n",
      "update merge graph 0.9075384140014648\n",
      "stop merging nodes 0.0887448787689209\n",
      "update merge graph 0.13606929779052734\n",
      "stop lowest cost edges 0.006646156311035156\n"
     ]
    }
   ],
   "source": [
    "for i in range(30):\n",
    "    coarsener.iteration_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "coarsend_graph = coarsener.get_coarsend_graph()\n",
    "mapping = coarsener.get_mapping(\"author\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'author': 4057, 'conference': 20, 'paper': 14328, 'term': 7723},\n",
       "      num_edges={('author', 'authortopaper', 'paper'): 19645, ('conference', 'conferencetopaper', 'paper'): 14328, ('paper', 'papertoauthor', 'author'): 19645, ('paper', 'papertoconference', 'conference'): 14328, ('paper', 'papertoterm', 'term'): 85810, ('term', 'termtopaper', 'paper'): 85810},\n",
       "      metagraph=[('author', 'paper', 'authortopaper'), ('paper', 'author', 'papertoauthor'), ('paper', 'conference', 'papertoconference'), ('paper', 'term', 'papertoterm'), ('conference', 'paper', 'conferencetopaper'), ('term', 'paper', 'termtopaper')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes={'author': 3877, 'conference': 9, 'paper': 14148, 'term': 7543},\n",
       "      num_edges={('author', 'authortopaper', 'paper'): 19144, ('conference', 'conferencetopaper', 'paper'): 14023, ('paper', 'papertoauthor', 'author'): 19237, ('paper', 'papertoconference', 'conference'): 2957, ('paper', 'papertoterm', 'term'): 85348, ('term', 'termtopaper', 'paper'): 84897},\n",
       "      metagraph=[('author', 'paper', 'authortopaper'), ('paper', 'author', 'papertoauthor'), ('paper', 'conference', 'papertoconference'), ('paper', 'term', 'papertoterm'), ('conference', 'paper', 'conferencetopaper'), ('term', 'paper', 'termtopaper')])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarsend_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio 0.9789115125535823\n"
     ]
    }
   ],
   "source": [
    "print(\"ratio\", coarsend_graph.num_nodes()/ original_graph.num_nodes() ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data, o_x_dict, o_edge_index_dict, o_node_types, o_edge_types = dgl_to_pyg_input(original_graph)\n",
    "coarsened_data, c_x_dict, c_edge_index_dict, c_node_types, c_edge_types = dgl_to_pyg_input(coarsend_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split statistics:\n",
      "  Training: 405 nodes (9.98%)\n",
      "  Validation: 405 nodes (9.98%)\n",
      "  Testing: 3247 nodes (80.03%)\n",
      "Split statistics:\n",
      "  Training: 387 nodes (9.98%)\n",
      "  Validation: 387 nodes (9.98%)\n",
      "  Testing: 3103 nodes (80.04%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# Assuming you already have your hetero_data object\n",
    "# hetero_data = HeteroData(...)\n",
    "\n",
    "def create_train_val_test_masks(hetero_data, train_ratio=0.1, val_ratio=0.1, test_ratio=0.8, random_state=42, target_node_type='author'):\n",
    "    \"\"\"\n",
    "    Create training, validation, and testing masks for author nodes.\n",
    "    \n",
    "    Args:\n",
    "        hetero_data: HeteroData object\n",
    "        train_ratio: Ratio of training data\n",
    "        val_ratio: Ratio of validation data\n",
    "        test_ratio: Ratio of testing data\n",
    "        random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        HeteroData object with train_mask, val_mask, and test_mask added to author nodes\n",
    "    \"\"\"\n",
    "    # Ensure ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1\"\n",
    "    \n",
    "    # Get number of author nodes\n",
    "    num_authors = hetero_data[target_node_type][\"feat\"].size(0)\n",
    "    \n",
    "    # Create random permutation of node indices\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(num_authors)\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    train_size = int(num_authors * train_ratio)\n",
    "    val_size = int(num_authors * val_ratio)\n",
    "    \n",
    "    # Split indices\n",
    "    train_indices = indices[:train_size]\n",
    "    val_indices = indices[train_size:train_size + val_size]\n",
    "    test_indices = indices[train_size + val_size:]\n",
    "    \n",
    "    # Create boolean masks\n",
    "    train_mask = torch.zeros(num_authors, dtype=torch.bool)\n",
    "    val_mask = torch.zeros(num_authors, dtype=torch.bool)\n",
    "    test_mask = torch.zeros(num_authors, dtype=torch.bool)\n",
    "    \n",
    "    train_mask[train_indices] = True\n",
    "    val_mask[val_indices] = True\n",
    "    test_mask[test_indices] = True\n",
    "    \n",
    "    # Add masks to hetero_data\n",
    "    hetero_data[target_node_type].train_mask = train_mask\n",
    "    hetero_data[target_node_type].val_mask = val_mask\n",
    "    hetero_data[target_node_type].test_mask = test_mask\n",
    "    \n",
    "    # Print split statistics\n",
    "    print(f\"Split statistics:\")\n",
    "    print(f\"  Training: {train_mask.sum()} nodes ({train_mask.sum() / num_authors:.2%})\")\n",
    "    print(f\"  Validation: {val_mask.sum()} nodes ({val_mask.sum() / num_authors:.2%})\")\n",
    "    print(f\"  Testing: {test_mask.sum()} nodes ({test_mask.sum() / num_authors:.2%})\")\n",
    "    \n",
    "    return hetero_data\n",
    "\n",
    "# Example usage\n",
    "original_data =  create_train_val_test_masks(original_data, train_ratio=0.1, val_ratio=0.1, test_ratio=0.8, target_node_type='author')\n",
    "coarsened_data = create_train_val_test_masks(coarsened_data, train_ratio=0.1, val_ratio=0.1, test_ratio=0.8, target_node_type='author')\n",
    "\n",
    "# Accessing the masks\n",
    "# train_nodes = hetero_data['author'].x[hetero_data['author'].train_mask]\n",
    "# val_nodes = hetero_data['author'].x[hetero_data['author'].val_mask]\n",
    "# test_nodes = hetero_data['author'].x[hetero_data['author'].test_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('author', 'authortopaper', 'paper'): tensor([[    0,     0,     1,  ...,  4054,  4055,  4056],\n",
      "        [ 2364,  6457,  2365,  ..., 13891, 13891, 13892]]), ('conference', 'conferencetopaper', 'paper'): tensor([[    0,     0,     0,  ...,    19,    19,    19],\n",
      "        [    0,     1,     2,  ..., 14325, 14326, 14327]]), ('paper', 'papertoauthor', 'author'): tensor([[    0,     1,     2,  ..., 14327, 14327, 14327],\n",
      "        [  262,   263,   263,  ...,   324,  1068,  3647]]), ('paper', 'papertoconference', 'conference'): tensor([[    0,     1,     2,  ..., 14325, 14326, 14327],\n",
      "        [    0,     0,     0,  ...,    19,    19,    19]]), ('paper', 'papertoterm', 'term'): tensor([[    0,     0,     0,  ..., 14327, 14327, 14327],\n",
      "        [    4,     5,     6,  ...,   586,   730,  1311]]), ('term', 'termtopaper', 'paper'): tensor([[   0,    0,    0,  ..., 7720, 7721, 7722],\n",
      "        [  19,   30,  225,  ..., 5166, 5168, 5174]])}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype, but got Double and Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 135\u001b[0m\n\u001b[1;32m    124\u001b[0m results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minverted_acc\u001b[39m\u001b[38;5;124m\"\u001b[39m: []\n\u001b[1;32m    131\u001b[0m }\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# Train both models\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     original_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_original\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     coarsened_loss \u001b[38;5;241m=\u001b[39m train_coarsened()\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Store results\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 66\u001b[0m, in \u001b[0;36mtrain_original\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m optimizer_original\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(original_data\u001b[38;5;241m.\u001b[39medge_index_dict)\n\u001b[0;32m---> 66\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_original\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_x_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(\n\u001b[1;32m     68\u001b[0m     out[target_node_type][original_data[target_node_type]\u001b[38;5;241m.\u001b[39mtrain_mask], \n\u001b[1;32m     69\u001b[0m     original_data[target_node_type][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m][original_data[target_node_type]\u001b[38;5;241m.\u001b[39mtrain_mask]\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     71\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/thesis/convolution-matching/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/convolution-matching/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/convolution-matching/Models/GNNs/HGCN.py:86\u001b[0m, in \u001b[0;36mImprovedHeteroGNN.forward\u001b[0;34m(self, x_dict, edge_index_dict)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_dict, edge_index_dict):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Initial embedding of node features\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m {node_type: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings[node_type](x) \n\u001b[1;32m     87\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m node_type, x \u001b[38;5;129;01min\u001b[39;00m x_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Apply multiple layers of heterogeneous graph convolutions\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;66;03m# Store previous embeddings for residual connections\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/convolution-matching/Models/GNNs/HGCN.py:86\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_dict, edge_index_dict):\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# Initial embedding of node features\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     x_dict \u001b[38;5;241m=\u001b[39m {node_type: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     87\u001b[0m              \u001b[38;5;28;01mfor\u001b[39;00m node_type, x \u001b[38;5;129;01min\u001b[39;00m x_dict\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# Apply multiple layers of heterogeneous graph convolutions\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;66;03m# Store previous embeddings for residual connections\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/convolution-matching/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/convolution-matching/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/thesis/convolution-matching/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype, but got Double and Float"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries (if not already imported)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from Models.GNNs.HGCN import ImprovedHeteroGNN\n",
    "\n",
    "\n",
    "original_x_dict = o_x_dict\n",
    "coarsened_x_dict = c_x_dict\n",
    "o_metadata = (o_node_types, o_edge_types)\n",
    "c_metadata=(c_node_types, c_edge_types)\n",
    "#original_x_dict.update({\"conference\": torch.zeros((original_data[\"conference\"][\"num_nodes\"], 1))})# = 0  #= 0#\n",
    "#coarsened_x_dict.update({\"conference\": torch.zeros((coarsened_data[\"conference\"][\"num_nodes\"], 1))})# = 0  #= 0#\n",
    "\n",
    "#num_classes = len(original_data[\"author\"][\"label\"].unique())\n",
    "\n",
    "target_node_type = \"author\"\n",
    "# Your existing model definition\n",
    "model_original = ImprovedHeteroGNN(metadata=o_metadata,target_feat=target_node_type, x_dict= original_x_dict ,num_classes= 4,hidden_channels=64, with_non_linear= False)\n",
    "model_coarsened = ImprovedHeteroGNN(metadata=c_metadata, target_feat=target_node_type,x_dict= coarsened_x_dict,num_classes= 4, hidden_channels=64, with_non_linear= False)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_original = torch.optim.Adam(model_original.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "optimizer_coarsened = torch.optim.Adam(model_coarsened.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "# Function to apply inversion mapping from coarsened to original\n",
    "def apply_inversion_mapping(coarsened_pred, inversion_map, original_labels, test_mask):\n",
    "    \"\"\"\n",
    "    Maps predictions from coarsened graph back to original graph nodes\n",
    "    \n",
    "    Args:\n",
    "        coarsened_pred: Predictions on coarsened graph\n",
    "        inversion_map: Mapping from coarsened nodes to original nodes\n",
    "        original_labels: Ground truth labels of original graph\n",
    "        \n",
    "    Returns:\n",
    "        mapped_accuracy: Accuracy after applying inversion mapping\n",
    "    \"\"\"\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for orig_node, coarsened_node in inversion_map.items():\n",
    "        if not test_mask[coarsened_node]:\n",
    "            continue\n",
    "        coarse_pred = coarsened_pred[coarsened_node]\n",
    "        orig_label = original_labels[orig_node]\n",
    "        #print(coarse_pred, orig_label)\n",
    "\n",
    "        correct = (coarse_pred == orig_label)\n",
    "    \n",
    "                \n",
    "        if correct:\n",
    "            correct_count += 1\n",
    "     #   else:\n",
    "      #      print(f\"Mismatch: Coarsened Node {coarsened_node} predicted {coarse_pred}, Original Node {orig_node} label {orig_label}\")\n",
    "        total_count += 1\n",
    "    \n",
    "    return correct_count / total_count if total_count > 0 else 0\n",
    "\n",
    "# Training function for original graph\n",
    "def train_original():\n",
    "    model_original.train()\n",
    "    optimizer_original.zero_grad()\n",
    "    \n",
    "    #print(original_x_original_data.edge_index_dict)\n",
    "    out = model_original(original_x_dict, original_data.edge_index_dict)\n",
    "    loss = F.nll_loss(\n",
    "        out[target_node_type][original_data[target_node_type].train_mask], \n",
    "        original_data[target_node_type][\"label\"][original_data[target_node_type].train_mask]\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer_original.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Training function for coarsened graph\n",
    "def train_coarsened():\n",
    "    model_coarsened.train()\n",
    "    optimizer_coarsened.zero_grad()\n",
    "    out = model_coarsened(coarsened_x_dict, coarsened_data.edge_index_dict)\n",
    "    loss = F.nll_loss(\n",
    "        out[target_node_type][coarsened_data[target_node_type].train_mask], \n",
    "        coarsened_data[target_node_type][\"label\"][coarsened_data[target_node_type].train_mask]\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer_coarsened.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Evaluation function for original graph\n",
    "def test_original():\n",
    "    model_original.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model_original(original_x_dict, original_data.edge_index_dict)\n",
    "        pred = out[target_node_type].argmax(dim=1)\n",
    "        \n",
    "        # Calculate accuracy on test set\n",
    "        correct = pred[original_data[target_node_type].test_mask] == original_data[target_node_type][\"label\"][original_data[target_node_type].test_mask]\n",
    "        acc = int(correct.sum()) / int(original_data[target_node_type].test_mask.sum())\n",
    "        return acc\n",
    "\n",
    "# Evaluation function for coarsened graph\n",
    "def test_coarsened():\n",
    "    model_coarsened.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model_coarsened(coarsened_x_dict, coarsened_data.edge_index_dict)\n",
    "        pred = out[target_node_type].argmax(dim=1)\n",
    "        \n",
    "        # Calculate accuracy on test set\n",
    "        coarsened_acc = int(\n",
    "            (pred[coarsened_data[target_node_type].test_mask] == \n",
    "             coarsened_data[target_node_type][\"label\"][coarsened_data[target_node_type].test_mask]).sum()\n",
    "        ) / int(coarsened_data[target_node_type].test_mask.sum())\n",
    "        inverted_acc = 0\n",
    "        \n",
    "        # Apply inversion mapping to evaluate how coarsened predictions map to original graph\n",
    "        inverted_acc  = apply_inversion_mapping(\n",
    "             pred,  \n",
    "             mapping,\n",
    "             original_data[target_node_type][\"label\"], \n",
    "             coarsened_data[target_node_type].test_mask\n",
    "         )\n",
    "        return coarsened_acc, inverted_acc\n",
    "\n",
    "# Training for 50 epochs and comparing both models\n",
    "results = {\n",
    "    \"epoch\": [],\n",
    "    \"original_loss\": [],\n",
    "    \"coarsened_loss\": [],\n",
    "    \"original_acc\": [],\n",
    "    \"coarsened_acc\": [],\n",
    "    \"inverted_acc\": []\n",
    "}\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Train both models\n",
    "    original_loss = train_original()\n",
    "    coarsened_loss = train_coarsened()\n",
    "    \n",
    "    # Store results\n",
    "    results[\"epoch\"].append(epoch+1)\n",
    "    results[\"original_loss\"].append(original_loss)\n",
    "    results[\"coarsened_loss\"].append(coarsened_loss)\n",
    "    \n",
    "    # Evaluate every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        original_acc = test_original()\n",
    "        coarsened_acc, inverted_acc = test_coarsened()\n",
    "        \n",
    "        results[\"original_acc\"].append(original_acc)\n",
    "        results[\"coarsened_acc\"].append(coarsened_acc)\n",
    "        results[\"inverted_acc\"].append(inverted_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}:')\n",
    "    \n",
    "    \n",
    "        print(f'  Original Graph - Loss: {original_loss:.4f}, Accuracy: {original_acc:.4f}')\n",
    "        print(f'  Coarsened Graph - Loss: {coarsened_loss:.4f}, Accuracy: {coarsened_acc:.4f}')\n",
    "        print(f'  Inverted Coarsened - Accuracy: {inverted_acc:.4f}')\n",
    "        print()\n",
    "\n",
    "# Visualization of results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(results[\"epoch\"], results[\"original_loss\"], label=\"Original Graph\")\n",
    "plt.plot(results[\"epoch\"], results[\"coarsened_loss\"], label=\"Coarsened Graph\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "x = [results[\"epoch\"][i] for i in range(0, 50, 10)]\n",
    "plt.plot(x, results[\"original_acc\"], 'o-', label=\"Original Graph\")\n",
    "plt.plot(x, results[\"coarsened_acc\"], 's-', label=\"Coarsened Graph\")\n",
    "plt.plot(x, results[\"inverted_acc\"], '^-', label=\"Inverted Coarsened\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Test Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final comparison\n",
    "print(\"Final Performance Comparison:\")\n",
    "print(f\"Original Graph Accuracy: {results['original_acc'][-1]:.4f}\")\n",
    "print(f\"Coarsened Graph Accuracy: {results['coarsened_acc'][-1]:.4f}\")\n",
    "print(f\"Inverted Coarsened Accuracy: {results['inverted_acc'][-1]:.4f}\")\n",
    "\n",
    "# Calculate speedup from using coarsened graph\n",
    "# (You would need to time the training for a proper comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvconv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
